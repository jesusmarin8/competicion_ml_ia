{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ccs          adduct   desc_1      desc_2   desc_3     desc_4  \\\n",
      "0  155.71001   Monomer_[M+H] -5.27492   420.85751  0.79886   60.68518   \n",
      "1  179.56000   Monomer_[M-H] -2.00000  1171.97156  1.13333  121.96276   \n",
      "2  180.00000  Monomer_[M+Na] -0.70060  1447.22644  1.30616  141.10509   \n",
      "3  155.53999  Monomer_[M+Na]  0.00000   479.80600  0.92774   74.90678   \n",
      "4  173.50000   Monomer_[M+H] -1.63669   978.85028  1.24843  112.27611   \n",
      "\n",
      "    desc_5     desc_6     desc_7    desc_8  ...  fgp_611  fgp_612  fgp_613  \\\n",
      "0  0.22702   54.36304   55.73885   8.28095  ...        0        0        1   \n",
      "1  0.15520  100.45390   97.68512  11.36667  ...        1        0        0   \n",
      "2  0.13916  110.47855  108.96603  12.96667  ...        0        1        1   \n",
      "3  0.21166   65.64590   66.68343   6.83333  ...        1        0        1   \n",
      "4  0.15789   87.88085   89.68015  10.73333  ...        0        1        0   \n",
      "\n",
      "   fgp_614  fgp_615  fgp_616  fgp_617  fgp_618  fgp_619  fgp_620  \n",
      "0        0        1        1        1        0        1        0  \n",
      "1        1        0        1        0        1        1        0  \n",
      "2        0        1        0        0        0        0        1  \n",
      "3        1        1        0        1        1        0        0  \n",
      "4        1        0        0        1        0        1        0  \n",
      "\n",
      "[5 rows x 1942 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"data/public_train.csv\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           adduct   desc_1       desc_2   desc_3     desc_4   desc_5  \\\n",
      "0   Monomer_[M+H] -2.52359    942.17682  1.29512   87.64198  0.16297   \n",
      "1   Monomer_[M-H]  2.00538  12122.62988  1.80601  621.73560  0.07288   \n",
      "2   Monomer_[M-H]  2.73984   6543.37451  1.52727  440.95099  0.08565   \n",
      "3  Monomer_[M+Na] -2.34683    695.94104  1.04344   74.62116  0.17652   \n",
      "4   Monomer_[M+H]  0.00000    101.73023  0.03813   20.08101  0.35359   \n",
      "\n",
      "      desc_6     desc_7    desc_8   desc_9  ...  fgp_611  fgp_612  fgp_613  \\\n",
      "0   83.92964   84.34066  11.00000  4.80405  ...        0        1        0   \n",
      "1  545.80798  554.61530  22.73333  5.64051  ...        0        0        0   \n",
      "2  305.62857  313.87540  19.90000  5.31395  ...        1        1        1   \n",
      "3   65.28460   63.78675  10.81429  4.25346  ...        1        0        0   \n",
      "4   19.35335   20.59024   6.06667  2.62874  ...        1        1        0   \n",
      "\n",
      "   fgp_614  fgp_615  fgp_616  fgp_617  fgp_618  fgp_619  fgp_620  \n",
      "0        1        1        1        1        1        0        1  \n",
      "1        1        0        1        1        1        1        1  \n",
      "2        0        0        0        0        0        1        0  \n",
      "3        1        1        0        0        0        1        1  \n",
      "4        0        1        1        0        1        1        1  \n",
      "\n",
      "[5 rows x 1941 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"data/public_test.csv\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya visaulizados los datos, se procede a realizar el preprocesamiento de los mismos. Para ello, se realizarán las siguientes tareas:\n",
    "- Eliminación de columnas innecesarias(Ingeniería de características)\n",
    "- Imputación\n",
    "- Codificación de variables categóricas\n",
    "- Normalización de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscaler = StandardScaler()#para las caracteristicas numericas\\nnumeric_imputer = SimpleImputer(strategy=\"mean\")#Para los nans de caracteristicas numericas\\ncategorical_imputer = SimpleImputer(strategy=\"most_frequent\")# Para los nans de caracteristicas categoricas\\noh_encoder = OneHotEncoder(sparse_output=False)# para las caracteristicas categoricas\\n\\nnumeric_transformer = Pipeline(steps=[\\n    (\"imputer\", numeric_imputer),\\n    (\"scaler\", scaler)\\n])\\n\\ncategorical_transformer = Pipeline(steps=[\\n    (\"imputer\", categorical_imputer),\\n    (\"encoder\", oh_encoder)\\n])\\n\\n\\ntransformer = ColumnTransformer(\\n    transformers=[\\n        (\\'numeric_imp\\', numeric_transformer, [\\'Age\\', \\'FamSize\\', \\'Fare\\']),  \\n        (\\'categorial_imp\\', categorical_transformer, [\\'Pclass\\', \\'Sex\\', \\'Embarked\\']) \\n    ]\\n)\\n\\ntransformer = transformer.set_output(transform=\"pandas\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#Codificación de la columna adduct: Convierte la columna categórica adduct en variables numéricas usando técnicas como One-Hot Encoding o Label Encoding.\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output = False)\n",
    "adduct_encoded = encoder.fit_transform(train[['adduct']])\n",
    "train_encoded = pd.concat([train.drop(columns=['adduct']), pd.DataFrame(adduct_encoded)], axis=1)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(train_encoded.drop(columns=['ccs']).values)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "scaler = StandardScaler()#para las caracteristicas numericas\n",
    "numeric_imputer = SimpleImputer(strategy=\"mean\")#Para los nans de caracteristicas numericas\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")# Para los nans de caracteristicas categoricas\n",
    "oh_encoder = OneHotEncoder(sparse_output=False)# para las caracteristicas categoricas\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", numeric_imputer),\n",
    "    (\"scaler\", scaler)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", categorical_imputer),\n",
    "    (\"encoder\", oh_encoder)\n",
    "])\n",
    "\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric_imp', numeric_transformer, ['Age', 'FamSize', 'Fare']),  \n",
    "        ('categorial_imp', categorical_transformer, ['Pclass', 'Sex', 'Embarked']) \n",
    "    ]\n",
    ")\n",
    "\n",
    "transformer = transformer.set_output(transform=\"pandas\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO Y ESTIMACIÓN DE ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de realizar el preprocesamiento de los datos, se procederá a entrenar un modelo de regresión logística y a estimar el error del modelo. Para ello, se realizarán las siguientes tareas:\n",
    "- División de los datos en conjuntos de entrenamiento y prueba\n",
    "- Entrenamiento del modelo\n",
    "- Estimación del error del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.348e+05, tolerance: 2.162e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Claudia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.320e+05, tolerance: 2.161e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Claudia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+05, tolerance: 2.171e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Claudia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.368e+05, tolerance: 2.156e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Claudia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+05, tolerance: 2.165e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio MDAE en validación cruzada: 3.3047503625525336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Claudia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+05, tolerance: 2.704e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDAE en conjunto de validación final: 3.097668908036013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np  # Necesario para calcular el promedio de los errores\n",
    "\n",
    "# Asegúrate de que los nombres de las columnas sean cadenas\n",
    "X = train_encoded.drop(columns=['ccs'])\n",
    "y = train_encoded['ccs']\n",
    "\n",
    "# Asegurar que X sea un DataFrame con columnas como cadenas\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Definir el pipeline mejorado\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('feature_selection', SelectFromModel(Lasso(alpha=0.005, random_state=42), threshold='median')),\n",
    "    ('model', GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42))  # Modelo optimizado\n",
    "])\n",
    "\n",
    "# Definir el scorer para MDAE\n",
    "scorer = make_scorer(median_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Realizar validación cruzada con KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Aumentado a 5 pliegues\n",
    "\n",
    "# Lista para almacenar los errores en cada pliegue\n",
    "cv_errors = []\n",
    "\n",
    "# Realizar validación cruzada\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    # Dividir los datos en entrenamiento y validación para cada pliegue\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Entrenamiento del pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción en el conjunto de validación\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "\n",
    "    # Calcular MDAE en el conjunto de validación\n",
    "    medae = median_absolute_error(y_val, y_pred)\n",
    "    cv_errors.append(medae)\n",
    "\n",
    "# Promedio de los errores de validación cruzada\n",
    "average_medae = np.mean(cv_errors)\n",
    "print(\"Promedio MDAE en validación cruzada:\", average_medae)\n",
    "\n",
    "# Entrenamiento del pipeline en todo el conjunto de entrenamiento\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predicción y evaluación final\n",
    "final_predictions = pipeline.predict(X_val)\n",
    "final_medae = median_absolute_error(y_val, final_predictions)\n",
    "print(\"MDAE en conjunto de validación final:\", final_medae)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERACIÓN DE PREDICCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 3: Generación de predicciones\n",
    "\n",
    "# Codificar la columna \"adduct\" en el conjunto de prueba con el mismo encoder\n",
    "adduct_test_encoded = encoder.transform(test[['adduct']])\n",
    "adduct_test_encoded = pd.DataFrame(adduct_test_encoded, columns=encoder.get_feature_names_out(['adduct']))\n",
    "\n",
    "# Combinar el conjunto de prueba con las columnas codificadas\n",
    "test_encoded = pd.concat([test.drop(columns=['adduct']), adduct_test_encoded], axis=1)\n",
    "\n",
    "# Alinear las columnas de test_encoded con las de X_train\n",
    "test_encoded = test_encoded.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Rellenar valores nulos\n",
    "test_encoded = test_encoded.fillna(test_encoded.mean())\n",
    "\n",
    "# Normalizar las características del conjunto de prueba con el mismo escalador\n",
    "X_test = scaler.transform(test_encoded)\n",
    "\n",
    "# Generar predicciones en el conjunto de prueba\n",
    "test_preds = pipeline.predict(X_test)\n",
    "\n",
    "# Guardar las predicciones en un archivo CSV\n",
    "pd.DataFrame(test_preds).to_csv(\"test_preds.csv\", index=False, header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONES "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
