{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ccs          adduct   desc_1      desc_2   desc_3     desc_4  \\\n",
      "0  155.71001   Monomer_[M+H] -5.27492   420.85751  0.79886   60.68518   \n",
      "1  179.56000   Monomer_[M-H] -2.00000  1171.97156  1.13333  121.96276   \n",
      "2  180.00000  Monomer_[M+Na] -0.70060  1447.22644  1.30616  141.10509   \n",
      "3  155.53999  Monomer_[M+Na]  0.00000   479.80600  0.92774   74.90678   \n",
      "4  173.50000   Monomer_[M+H] -1.63669   978.85028  1.24843  112.27611   \n",
      "\n",
      "    desc_5     desc_6     desc_7    desc_8  ...  fgp_611  fgp_612  fgp_613  \\\n",
      "0  0.22702   54.36304   55.73885   8.28095  ...        0        0        1   \n",
      "1  0.15520  100.45390   97.68512  11.36667  ...        1        0        0   \n",
      "2  0.13916  110.47855  108.96603  12.96667  ...        0        1        1   \n",
      "3  0.21166   65.64590   66.68343   6.83333  ...        1        0        1   \n",
      "4  0.15789   87.88085   89.68015  10.73333  ...        0        1        0   \n",
      "\n",
      "   fgp_614  fgp_615  fgp_616  fgp_617  fgp_618  fgp_619  fgp_620  \n",
      "0        0        1        1        1        0        1        0  \n",
      "1        1        0        1        0        1        1        0  \n",
      "2        0        1        0        0        0        0        1  \n",
      "3        1        1        0        1        1        0        0  \n",
      "4        1        0        0        1        0        1        0  \n",
      "\n",
      "[5 rows x 1942 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"data/public_train.csv\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           adduct   desc_1       desc_2   desc_3     desc_4   desc_5  \\\n",
      "0   Monomer_[M+H] -2.52359    942.17682  1.29512   87.64198  0.16297   \n",
      "1   Monomer_[M-H]  2.00538  12122.62988  1.80601  621.73560  0.07288   \n",
      "2   Monomer_[M-H]  2.73984   6543.37451  1.52727  440.95099  0.08565   \n",
      "3  Monomer_[M+Na] -2.34683    695.94104  1.04344   74.62116  0.17652   \n",
      "4   Monomer_[M+H]  0.00000    101.73023  0.03813   20.08101  0.35359   \n",
      "\n",
      "      desc_6     desc_7    desc_8   desc_9  ...  fgp_611  fgp_612  fgp_613  \\\n",
      "0   83.92964   84.34066  11.00000  4.80405  ...        0        1        0   \n",
      "1  545.80798  554.61530  22.73333  5.64051  ...        0        0        0   \n",
      "2  305.62857  313.87540  19.90000  5.31395  ...        1        1        1   \n",
      "3   65.28460   63.78675  10.81429  4.25346  ...        1        0        0   \n",
      "4   19.35335   20.59024   6.06667  2.62874  ...        1        1        0   \n",
      "\n",
      "   fgp_614  fgp_615  fgp_616  fgp_617  fgp_618  fgp_619  fgp_620  \n",
      "0        1        1        1        1        1        0        1  \n",
      "1        1        0        1        1        1        1        1  \n",
      "2        0        0        0        0        0        1        0  \n",
      "3        1        1        0        0        0        1        1  \n",
      "4        0        1        1        0        1        1        1  \n",
      "\n",
      "[5 rows x 1941 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"data/public_test.csv\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya visaulizados los datos, se procede a realizar el preprocesamiento de los mismos. Para ello, se realizarán las siguientes tareas:\n",
    "- Eliminación de columnas innecesarias(Ingeniería de características)\n",
    "- Imputación\n",
    "- Codificación de variables categóricas\n",
    "- Normalización de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscaler = StandardScaler()#para las caracteristicas numericas\\nnumeric_imputer = SimpleImputer(strategy=\"mean\")#Para los nans de caracteristicas numericas\\ncategorical_imputer = SimpleImputer(strategy=\"most_frequent\")# Para los nans de caracteristicas categoricas\\noh_encoder = OneHotEncoder(sparse_output=False)# para las caracteristicas categoricas\\n\\nnumeric_transformer = Pipeline(steps=[\\n    (\"imputer\", numeric_imputer),\\n    (\"scaler\", scaler)\\n])\\n\\ncategorical_transformer = Pipeline(steps=[\\n    (\"imputer\", categorical_imputer),\\n    (\"encoder\", oh_encoder)\\n])\\n\\n\\ntransformer = ColumnTransformer(\\n    transformers=[\\n        (\\'numeric_imp\\', numeric_transformer, [\\'Age\\', \\'FamSize\\', \\'Fare\\']),  \\n        (\\'categorial_imp\\', categorical_transformer, [\\'Pclass\\', \\'Sex\\', \\'Embarked\\']) \\n    ]\\n)\\n\\ntransformer = transformer.set_output(transform=\"pandas\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#Codificación de la columna adduct: Convierte la columna categórica adduct en variables numéricas usando técnicas como One-Hot Encoding o Label Encoding.\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output = False)\n",
    "adduct_encoded = encoder.fit_transform(train[['adduct']])\n",
    "train_encoded = pd.concat([train.drop(columns=['adduct']), pd.DataFrame(adduct_encoded)], axis=1)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(train_encoded.drop(columns=['ccs']).values)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "scaler = StandardScaler()#para las caracteristicas numericas\n",
    "numeric_imputer = SimpleImputer(strategy=\"mean\")#Para los nans de caracteristicas numericas\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")# Para los nans de caracteristicas categoricas\n",
    "oh_encoder = OneHotEncoder(sparse_output=False)# para las caracteristicas categoricas\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", numeric_imputer),\n",
    "    (\"scaler\", scaler)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", categorical_imputer),\n",
    "    (\"encoder\", oh_encoder)\n",
    "])\n",
    "\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric_imp', numeric_transformer, ['Age', 'FamSize', 'Fare']),  \n",
    "        ('categorial_imp', categorical_transformer, ['Pclass', 'Sex', 'Embarked']) \n",
    "    ]\n",
    ")\n",
    "\n",
    "transformer = transformer.set_output(transform=\"pandas\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO Y ESTIMACIÓN DE ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de realizar el preprocesamiento de los datos, se procederá a entrenar un modelo de regresión logística y a estimar el error del modelo. Para ello, se realizarán las siguientes tareas:\n",
    "- División de los datos en conjuntos de entrenamiento y prueba\n",
    "- Entrenamiento del modelo\n",
    "- Estimación del error del modelo\n",
    "\n",
    "Usamos un pipeline para el preprocesamiento, selección de características y entrenamiento del modelo.\n",
    "Además , hacemos uso de la técnica de validación cruzada K-fold reduciendo el riesgo de sobreajuste debido a la división de los datos en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(random_state=42)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_index], y\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Entrenamiento del pipeline\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Predicción en el conjunto de validación\u001b[39;00m\n\u001b[0;32m     46\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:386\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, routed_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_steps()\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;66;03m# Setup the memory\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     memory \u001b[38;5;241m=\u001b[39m check_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:256\u001b[0m, in \u001b[0;36mPipeline._validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m    254\u001b[0m         t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m     ):\n\u001b[1;32m--> 256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll intermediate steps should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers and implement fit and transform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor be the string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (t, \u001b[38;5;28mtype\u001b[39m(t))\n\u001b[0;32m    261\u001b[0m         )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    265\u001b[0m     estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m ):\n",
      "\u001b[1;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'SMOTE(random_state=42)' (type <class 'imblearn.over_sampling._smote.base.SMOTE'>) doesn't"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import median_absolute_error , make_scorer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer \n",
    "import pandas as pd\n",
    "import numpy as np  # Necesario para calcular el promedio de los errores\n",
    "\n",
    "# Asegúrate de que los nombres de las columnas sean cadenas\n",
    "X = train_encoded.drop(columns=['ccs'])\n",
    "y = train_encoded['ccs']\n",
    "\n",
    "# Asegurar que X sea un DataFrame con columnas como cadenas\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Definir el pipeline mejorado\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('feature_selection', SelectFromModel(Lasso(alpha=0.005, random_state=42), threshold='median')),\n",
    "    ('model', GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42))  # Modelo optimizado\n",
    "])\n",
    "\n",
    "# Definir el scorer para MDAE\n",
    "scorer = make_scorer(median_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Realizar validación cruzada con KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Aumentado a 5 pliegues\n",
    "\n",
    "# Lista para almacenar los errores en cada pliegue\n",
    "cv_errors = []\n",
    "\n",
    "# Realizar validación cruzada\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    # Dividir los datos en entrenamiento y validación para cada pliegue\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Entrenamiento del pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción en el conjunto de validación\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "\n",
    "    # Calcular MDAE en el conjunto de validación\n",
    "    medae = median_absolute_error(y_val, y_pred)\n",
    "    cv_errors.append(medae)\n",
    "\n",
    "# Promedio de los errores de validación cruzada\n",
    "average_medae = np.mean(cv_errors)\n",
    "print(\"Promedio MDAE en validación cruzada:\", average_medae)\n",
    "\n",
    "# Entrenamiento del pipeline en todo el conjunto de entrenamiento\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predicción y evaluación final\n",
    "final_predictions = pipeline.predict(X_val)\n",
    "final_medae = median_absolute_error(y_val, final_predictions)\n",
    "print(\"MDAE en conjunto de validación final:\", final_medae)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERACIÓN DE PREDICCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 3: Generación de predicciones\n",
    "\n",
    "# Codificar la columna \"adduct\" en el conjunto de prueba con el mismo encoder\n",
    "adduct_test_encoded = encoder.transform(test[['adduct']])\n",
    "adduct_test_encoded = pd.DataFrame(adduct_test_encoded, columns=encoder.get_feature_names_out(['adduct']))\n",
    "\n",
    "# Combinar el conjunto de prueba con las columnas codificadas\n",
    "test_encoded = pd.concat([test.drop(columns=['adduct']), adduct_test_encoded], axis=1)\n",
    "\n",
    "# Alinear las columnas de test_encoded con las de X_train\n",
    "test_encoded = test_encoded.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Rellenar valores nulos\n",
    "test_encoded = test_encoded.fillna(test_encoded.mean())\n",
    "\n",
    "# Normalizar las características del conjunto de prueba con el mismo escalador\n",
    "X_test = scaler.transform(test_encoded)\n",
    "\n",
    "# Generar predicciones en el conjunto de prueba\n",
    "test_preds = pipeline.predict(X_test)\n",
    "\n",
    "# Guardar las predicciones en un archivo CSV\n",
    "pd.DataFrame(test_preds).to_csv(\"test_preds.csv\", index=False, header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONES Y JUSTIFICACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el inicio, convertirmos la columna categórica adduct en una representación numérica utilizando One-Hot Encoding.\n",
    "¿Por qué?, los modelos que estamos usando no trabajan con datos categóricos es decir \"letras\", necesitan que estas columnas sean transformadas en valores numéricos. Posteriormente, escalamos los datos para que todas las características tengan la misma importancia en el modelo usando RobustScaler ya que  es robusto frente a valores atípicos, es decir, no se ve afectado por valores extremos.Dentro del pipeline, se incluyó la selección de características utilizando el método de imputación de características estandar, que rellena los valores faltantes con la media de la columna, seguido de esto , se realiza la reguralización L1 para seleccionar las características más importantes y redciendo a 0 las menos importantes.¿Porque no usamos L2?, porque L1 es más efectiva, ya que reduce a 0 las menos importantes, mientras que L2 solo reduce su peso manteniendo TODAS las caracteristicas y finalmente en el pipeline se incluyó el modelo de regresión logística Gradient Boosting que trata de mejorar el rendimiento de los modelos de regresión logística mediante la combinación de varios modelos de regresión logística. Finalmente, se estimó el error del modelo utilizando la validación cruzada K-fold, que reduce el riesgo de sobreajuste debido a la división de los datos en conjuntos de entrenamiento y prueba. \n",
    "\n",
    "El modelo de regresión logística tiene un rendimiento aceptable para la predicción de la variable objetivo. Sin embargo, mejoraria el rendimiento del modelo mediante la optimización de los hiperparámetros del modelo, esto se implementó, sin embargo , la espera de tiempo de ejecución era muy larga, por lo que se decidió no incluirlo en el notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
